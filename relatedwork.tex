%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Related Work
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}
\label{sec:related}

Our research builds on previous work from two areas: 
\begin{inparaenum}[\itshape 1\upshape)]
\item Data quality management and
\item Statistical relational learning, namely probabilistic-logical languages.
\end{inparaenum}

\textbf{Data Quality Management}: A rich theoretical and practical investigation into data cleaning was presented in~\cite{Fan:2014:IRM:2628135.2567657} and \cite{fellegi1976systematic}. Fan W. et al.~\cite{Fan:2011:IRM:1989323.1989373} first proposed to unify the record matching and data repairing processes. They also proved that this unification greatly contributes to the improvement of the data quality. Based on this fundamental research, we propose to use probabilistic-logical frameworks to model the interaction of data quality rules and show that Markov logic enables us to simultaneously use MDs and CFDs. Holistic data cleaning methods based on integrity constraints and denial constraints with ad-hoc predicates have also been studied in \cite{chu2013holistic}. This approach considers the generalization of integrity constraints by translating them into denial constraints. However, denial constraints cannot express inclusion dependencies, hence in our work, we use the clausal form of the first-order predicate logic to express all kinds of integrity constraints. A generalization of dependencies was proposed by the Llunatic system in \cite{llunaticVDLB2013b}. They introduce a new language based on equality generated dependencies to standardize the way in which to express intra- and inter-dependencies. A clustering-based declarative approach for deduplication based on data constraints has been suggested in the Dedupalog language in \cite{Arasu:2009:LDC:1546683.1547340}.

The \textsc{Nadeeff} system from \cite{Dallachiesa:2013:NCD:2463676.2465327} is the system closest to ours with regard to the coverage of requirements for data cleaning systems. Analogous to our system, they treat data quality rules holistically. In contrast to \textsc{Nadeeff}, 
\begin{inparaenum}[\itshape 1\upshape)]
	\item we use first-order logic to define all the kinds of data quality rules and, therefore do not need black-boxes in the form of user-defined functions. Furthermore, we claim a higher usability of the system through ability to declaratively formulate rules;
	\item we perform data cleaning as an inference process on Markov networks. The inference is fully hidden from the user, which achieves a substantial complexity reduction.
\end{inparaenum}  

Relevant work in statistical inference and data cleaning has been conducted by Mayfield and his team in~\cite{Mayfield:2010:EDA:1807167.1807178}. Their system \textsc{Eracer} has been designed to perform missing values imputation. In our work, we also use statistical inference to predict missing values, repair data, and detect duplicate entries. We apply MAP inference to infer the types of errors and their sources. We also assume that the data quality rules based on FDs, CFDs, and MDs are already defined. Song and Chen, as well as Fan W. et al. present profiling algorithms to discover CFDs and MDs automatically~\cite{song2009discovering,Fan:2011:DCF:1978258.1978514}. Applying machine learning for entity deduplication has been demonstrated in \cite{guo2010record}. The work in \cite{beskales2010probclean} uses a probabilistic model for duplicate detection with uncertain outcomes. Throughout our work we use SRL, which is an extension of machine learning with respect to existing relations in the data. There are more approaches to combine logical and statistical data cleaning suggested by Prokoshyna N. et al in \cite{prokoshyna2015combining}, they apply relaxed functional dependencies (metric FDs \cite{Koudas2009MFD, caruccio2016relaxed}) as integrity constraint interface and propose a strategy to choose a high-quality minimal repair. Assessing the effectiveness of data cleaning by introducing the statistical distortion metric is investigated in \cite{dasu2012statistical}. 

The most recent work in data curation that subsumes data cleaning is the Data Timer System~\cite{Stonebraker_datacuration}. The researchers present an end-to-end system that performs massive data curation and data deduplication by combining two elements: machine learning and expert (human) feedback. In our work, we emphasize the joint execution of data cleaning rules, which are declared as \textit{soft} or \textit{hard}. Furthermore, we do not require human input to achieve high accuracy results, instead, we fully rely on the MAP-inference results to predict errors. Using machine learning and likelihood methods for cleaning noisy databases by predicting possible data updates has been introduced in \textsc{SCARE} system \cite{Yakout:2013:DSU:2463676.2463706}. In contrast to \textsc{SCARE}, our solution is capable of modeling and predicting not only missing values imputation and data consistency but also data deduplication.

Recently proposed declarative approach in \cite{burdick2015MLN} to data deduplication adopts \textit{link-to-source} constraints and theoretically proofs a connection of entity linking to a probabilistic framework based on MLN. In our work, we empirically show that MLN based data cleaning is a natural fit for solving data quality issues. 

\textbf{Statistical Relational Learning}: An advantage of probabilistic modeling for data quality has been investigated in~\cite{doi:10.1080/01621459.1972.10481323} and~\cite{chen2011usher}. Markov logic as a formalism for joint inference has been successfully used in a number of tasks, including natural language processing \cite{che2010jointly, riedel08collective, meza09jointly}, ontology alignment, data integration~\cite{niepert2011probabilistic} and co-reference resolution~\cite{poon2008joint,singla2006entity}. These research results demonstrate the advantage of joint modeling vs. pipeline execution. We are the first to apply this formalism to data cleaning and to show the benefits of a joint data cleaning approach that uses MLNs as a framework for interacting data quality rules. 

