
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Conclusion
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion and Future Work}
\label{sec:conclusion}
In this paper, we presented a declarative data-cleaning approach based on statistical relational learning and probabilistic inference. Generally, data quality rules represent relationships between attributes in the database schema and these rules are mainly based on functional dependencies on top of a database schema. We demonstrated how such functional dependencies, expressed as first-order logic formulas, can be translated into probabilistic logical languages, allowing us to reason over inconsistencies or duplicates in a probabilistic way. Our approach allows the usage of probabilistic joint inference over interleaved data cleaning rules to improve data quality. By using a declarative probabilistic-logical formalism such as Markov logic, we are potentially able to incorporate more semantic constraints (latent semantic factors) and, therefore, extend traditional data quality rules. We are currently extending the algorithm to massive data set settings and preliminary results are encouraging. With regards to experimenting with modeling additional semantic constraints, larger and more heterogeneous data sets, present and future research will focus on the improving data quality for distributed data. The results that we have presented in paper indicate that taking a holistic view on data cleaning and that modeling this intuition within a Markov logic framework is a feasible and effective means to create data cleaning systems. 