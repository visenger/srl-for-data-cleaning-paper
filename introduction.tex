%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Introduction
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:intro}

%What is the problem?
% see here for description why data quality is important.
% http://books.google.de/books?hl=en&lr=&id=SULMBFgtwQoC&oi=fnd&pg=PA1&ots=uaNGyVfL7V&sig=RER31QotCCRJeaQq8OafAtpgK1k#v=onepage&q&f=false
Having access to high quality data is of great importance in data analysis. However, in the real world, data is often considered \textit{dirty}, meaning that it contains inaccurate, incomplete, inconsistent, duplicated, or stale values ~\cite{chu2004blissful}. A number of distinct data quality issues are known in the field of Data Quality Management such as \textit{data consistency}, \textit{currency}, \textit{accuracy}, \textit{deduplication} and \textit{information completeness}~\cite{fan2012foundations}. As previous work has observed, such data quality issues may be detrimental to data analysis~\cite{national2013Frontiers,Fan:2008:CFD:1366102.1366103} and cause huge costs to businesses ~\cite{waynew.eckerson2002}. Therefore, improving data quality with respect to business and integrity constraints is a crucial component of data management. 
A common approach to increasing data quality is to formulate a set of \textit{data cleaning rules} that detect semantic errors by utilizing data dependencies~\cite{fan2012foundations, Arasu:2009:LDC:1546683.1547340, Dallachiesa:2013:NCD:2463676.2465327, llunaticVDLB2013b}. However, previous research identified a number of requirements and accompanying it challenges, which are associated with creating such rule sets: 

%\todo[inline]{Desiderata: Challenge}  
\textbf{Interleaved rules}. The first aspect of this requirement is, while each such rule may address one data quality issue individually, previous work in~\cite{fan2012foundations} and \cite{Fan:2014:IRM:2628135.2567657} has observed that such rules may \textit{interact}. For instance, a rule that deletes duplicates might perform better if missing data has already been imputed, while, on the other hand, a rule that imputes missing data might perform better if duplicates have already been removed. This means that data quality rules such as deduplication and missing value imputation should be modeled jointly, rather than as separate processes.
Secondly, rules in such a rule-set may need to be \textit{"soft"} and \textit{"hard"} in order to balance constraints of different importance \cite{Yakout:2013:DSU:2463676.2463706}, especially within a set of interacting rules. This makes the creation of such rule sets challenging from a modeling perspective. 


\textbf{Automation}. While interleaved data curation different order of rules execution produces different results \cite{Dallachiesa:2013:NCD:2463676.2465327}. This challenge conflicts with the automation principle of data curation systems suggested in \cite{Stonebraker_datacuration}. In this paper we tried to answer the question: is an optimal order of rules execution possible? 

\textbf{Usability and domain knowledge integration}. Various languages and statistical approaches for data curation exist \cite{Dallachiesa:2013:NCD:2463676.2465327, chu2013holistic, llunaticVDLB2013b} but there is a need for expressiveness and customization of the rules in order to integrate arbitrary constraints into data cleaning.  

In this paper, we present an approach to data cleaning based on Statistical Relational Learning (SRL)~\cite{getoor2007introduction} and probabilistic inference. SRL is a branch of machine learning that models joint distributions over relational data. Generally, data quality rules represent relationships between attributes in the database schema. These rules are mainly based on integrity constraints such as functional dependencies~\cite{AbiteboulHV95, fan2012foundations} on top of a database schema. We show how such functional dependencies, expressed as first-order logic formulas, can be translated into probabilistic-logical languages, allowing us to reason over inconsistencies, duplicates or missing values in a probabilistic way. While automatic data cleaning, the optimal order of rules execution is hardly achievable \cite{Dallachiesa:2013:NCD:2463676.2465327}, therefore we propose to use joint inference for the simultaneous rules execution. Our contribution we summarize as follows:

\begin{itemize}
	\item \textit{Execution Engine} We demonstrate the empirical study of holistic modeling and errors prediction in the data cleaning by using Markov Logic. Defining probabilistic data quality rules enables us to use statistical inference in order to predict errors. Our experiments, demonstrated in Sections \ref{subsec:exp1} and \ref{subsec:exp2}, reveal that simultaneous treatment of different data cleaning rules lead to the higher accuracy in data curation.  
	\item \textit{Backend/Compiler} We demonstrate how data cleaning problem is expressed as probabilistic graphical model. Using joint inference for data correction prediction (Section \ref{subsec:exp3}) outperforms sequential execution of data cleaning rules and results in improved accuracy of $0.95~F_1$-score.  
	\item \textit{Frontend} We demonstrate how data cleaning rules based on integrity constraints are modeled within the Markov logic (Section \ref{sec:method}). The usability aspect is discussed as a part of experiments in Section \ref{subsec:exp4}
\end{itemize}



\todo[inline]{Paper outline. Can we use our contribution to point to the paper outline?} 

