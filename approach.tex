
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Approach
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proposed Approach}
\label{sec:method}

%\note{This intro here must be much more concrete, it is too general and high level. Also, it is very long.}

 \begin{figure}[t]
 \centering
 \includegraphics[width=0.9\columnwidth]{img/system.pdf}
 \caption{%\anno{Rmove this picture?} Why? I find this pic useful and supports understanding.
Overview of the proposed data cleaning approach
 %\anno{Please redo this picture to make it look better (e.g., make sure the margins of text to borders are always the same.)} 
 %Our proposed method for data cleaning based on Markov logic consists of three main components: 
 %(I) A \textbf{Rule-based Data Quality Management} component that is based on the data quality rules to address different data quality issues;
 %(II) A \textbf{relation prediction} component based on probabilistic inference performed by the Markov logic framework and (III) 
 %a \textbf{Data Layer} that includes a number of data sources including relational and semi-structured data.}
 %\anno{Figure caption is too long} 
}
 \label{fig:system}
\end{figure}     

We propose to model data cleaning rules in the \textit{Markov logic}~\cite{domingos2009markov} formalism and use probabilistic inference for data cleaning. Markov logic is a knowledge representation system that combines first-order logic as a declarative language and probabilistic graphical models (undirected Markov Networks). On top of this representation we perform probabilistic inference. We introduce technical aspects of Markov logic and joint inference for data cleaning in Section~\ref{subsec:jointinference}.

The main advantage of Markov logic is the ability to reason simultaneously about complex and interacting relationships. By modeling different data cleaning rules as a Markov logic program, we leverage the joint inference on underlying probabilistic graphical model. In this way, we implement \textit{holistic} data cleaning, which is one of main requirements for data cleaning systems \cite{fan2013data, Fan:2014:IRM:2628135.2567657, Dallachiesa:2013:NCD:2463676.2465327}. If no perfect resolution of a rule set is possible, we want to find data cleaning steps that fulfill as many rules as possible while violating only a few~\cite{genesereth1987logical, domingos2009markov}. Therefore, in order to infer the types of errors and their sources, we consider integrity constraints to be probabilistic. We build on an existing approach to specify \textit{soft} functional dependencies between columns~\cite{Ilyas:2004:CAD:1007568.1007641}. 
%Another advantage of Markov Logic is that it allows us to tackle overfitting, a common problem when applying machine learning for repairing errorneous data \anno{You still need to explain overfitting!, EDBT reviewers might not be ML experts}. Overfitting occurs when one applies data quality rules that hold only for a fraction of data, but do not hold globally. Markov logic programs with \textit{"soft"} rules prohibit overfitting by adding weights to the constaint-based rules and incorporating partial knowledge \cite{singla2006entity, poon2008joint, lowd2007recursive}. \note{one more sentence, how does adding weights prohibit overfitting? do we show this experimentally somehow? if not, we might want to remove this paragraph}. 
Moreover, Markov logic enables us to extend traditional data quality rules with semantic constraints~\cite{spies2013knowledge}, meaning that additional knowledge (e.g., in form of predicates that model $\approx$, $\neq,~\leq,\geq$, etc.) can be incorporated into conventional data quality rules. This is very helpful in order to capture non-traditional dependencies \cite{chen2009analyses} for data cleaning. %\note{explain this in more detail, I don't understand what you mean}

In the following, we illustrate how to transfer data cleaning rules into Markov logic and leverage probabilistic inference to determine data repair operations for a given data set. See also Figure \ref{fig:system} for high-level overview of our approach. We distinguish between three major components: 
\begin{inparaenum}[\itshape I\upshape)]
\item \textit{Rule-based Data Quality Management} where we specify data-agnostic quality rules to address different data quality issues;
\item \textit{Statistical Inference with Markov Logic} to compute probabilistic data repair based on probabilistic inference performed by the Markov logic framework; and
\item \textit{Data Layer}, which interacts with former two components by providing both integrity constraints and evidence data from a number of data sources including relational and
semi-structured data for Markov logic.
\end{inparaenum}
%\note{YOU NEED TO EXPLAIN FIGURES, NOT JUST REFERENCE THEM *PEITSCH*} *AUTSCH*


\begin{figure*}[t]
 \centering
 \includegraphics[width=450px, height=200px]{img/mlogic-grounging.jpg}
 \caption{
 %\anno{Please redo this picture to make it look better} this will be really time consuming for now. If I find someone who can do that, I will let them produce sexier picture than this one.
 In context of data cleaning workflow, Markov Logic Network grounding process consists of two phases 
 1) MLN definition by 
(a) fixing MLN schema by defining observed and hidden predicates
(b) domain, which is created from the existing data by considering the MLN schema, and 
(c) specification of weighted first-order logic formulas that represent data cleaning rules;
 2) MLN instantiation by assigning truth values to the all possible instantiations of the MLN predicates by consideration of the domain (Random Variables) and using these ground atoms in formulas. These Ground Formulas constitutes Markov Network in order to compute MAP inference and to estimate the most possible data repair.}
 \label{fig:mlngrounding}
\end{figure*}

\subsection{Compilation of Data Cleaning Rules to \\Predicate Calculus}
\label{sec:ml} 

%\anno{Does your system automate the compilation? If yes, mention this!} - right now, there is a master thesis I am supervising, which implements the automate compilation. 
Next, we describe how to connect the data quality rules formalism to the Markov logic. The base of Markov logic programs is a predicate calculus \cite{genesereth1987logical}, because Markov logic consists of first-order logic sentences. Therefore, we describe a general method to compile formal constraint-based data cleaning rules into predicate calculus. We define data cleaning rules in the form of CFDs and MDs as introduced in Section~\ref{sec:expl}. For example, given the following functional dependency $\phi: X \rightarrow Y$, according to~\cite{Fagin:1982:HCD:322344.322347}, we express $\phi$ as \textit{first-order logic} sentence:
\begin{equation}
\mathsf{\forall x, y_1, y_2, z_1, z_2 \mathcal{R}(x, y_1, z_1) \wedge \mathcal{R}(x, y_2, z_2) \Rightarrow y_1=y_2}
\label{fd2fol}
\end{equation}

In following, we show that first-order logic sentence is crucial for the compilation of data cleaning rules into predictive models. % \note{Aha, why?} - well I think, to understand this, the following explanation is important.
Consider the logical equivalence of the data quality rule $\phi$ in \ref{fd2fol} as a composite component, consisting of subcomponents such as atomic sentences (attribute), logical and quantified sentences (RHS and LHS in FD $\phi$). To describe the structure of $\phi$ in a predicate calculus, we choose symbols that designate the elements of our conceptualization. In connection to the fundamentals of data quality management (c.f.,~Section~\ref{sec:expl}), we define the \textit{vocabulary} we use for the compilation of data quality rules: 
%\anno{WHY ANOTHER VOCABULARY? THIS IS SUPER CONFUSING!!! REWRITE THIS PART TO CONNECT TO THE REST OF THE PAPER}

The \textit{Universe of Disclosure} is specified by the set of all objects from domain $dom(U)$ that is fixed for the set of attributes $attr(\mathcal{R})$. A \textit{term} is used as a name for an object in the universe of discourse. We define \textit{variables} to denote arguments in atoms and \textit{constants} to denote data constants of particular domain $dom(U_i)$ of an $i$-th attribute $U_i \in attr(\mathcal{R})$. To designate the tuple in relation $\mathcal{R}$: $\mathcal{R}(x_1,x_2, \dots , x_n)$, we use atomic sentences $\mathsf{\textsl{attr-X}_1(id,v_1)}$, $\mathsf{\textsl{attr-X}_2(id,v_2)}$ $\dots$ $\mathsf{\textsl{attr-X}_n(id,v_n)}$ where $\mathsf{\textsl{attr-X}_i(id,v_i)}$ means that $\mathsf{v_i}$ is a attribute value of the $i$-th attribute in $\mathcal{R}(x_1,x_2, \dots , x_n)$ of the $id$-th tuple in relation $\mathcal{R}$. \textit{Relation constants} denote relations between several objects:
	\begin{itemize}
		\item Similarity: $\mathsf{\textsl{similar}(x_1,x_2)}$ means that $\mathsf{x_1}$ similar to $\mathsf{x_2}$ (e.g., by using different similarity measures like cosine or Jaccard similarities).
		\item Equality: $\mathsf{\textsl{equal-X}(id_1, id_2)}$ means that the values of the attribute X of two tuples $\mathsf{id_1}$ and $\mathsf{id_2}$ should be equal.
		\item Matching: $\mathsf{\textsl{match-X}(id_1, id_2)}$ means that values of two tuples $\mathsf{id_1}$ and $\mathsf{id_2}$ of the attribute X are identified to match.
		\item Custom Predicate: used for encoding diverse semantic constraints (e.g. \textit{contains}, \textit{between}, \textit{less} etc.), which are not part of the data constraints.
	\end{itemize}

%\note{Is the compilation more than a slightly different syntax? What are the challenges? This is one of the contributions, need more elaboration.} this is formalization, previous reviewer has asked for.
Given this vocabulary, we describe our conceptualization of the data quality rules with predicate-calculus sentences. For example, for the functional dependency $\phi: X \rightarrow Y$ and the first-order logic sentence in Formula \ref{fd2fol}, we compile to the corresponding Markov logic formula as follows:
\begin{flalign*}
  & \mathsf{\textsl{attr-X}(id_1, x) \wedge \textsl{attr-X}(id_2, x) \Rightarrow \textsl{equal-Y}(id_1, id_2)} & 
\end{flalign*}
\vspace*{-0.5cm}

Analogously, we translate a matching dependency $ \mu: \mathcal{S}_1[x_1]\approx \mathcal{S}_2[x_2]\rightarrow \mathcal{S}_1[y_1]\rightleftharpoons \mathcal{S}_2[y_2]$ on a database instance $\mathcal{D}$ with two relational schemas $\mathcal{S}_1$ and $\mathcal{S}_2$ as follows:
\begin{flalign*}
  & \mathsf{\textsl{attr-X/S1}(id_1, x_1) \wedge \textsl{attr-X/S2}(id_2, x_2) \wedge \textsl{similar}(x_1, x_2)} & \\
  & \mathsf{\Rightarrow \textsl{S1/match-Y/S2}(id_1, id_2)} & 
\end{flalign*}
\vspace*{-0.5cm}

If we specify a matching dependency using two relations, we need to mark the attributes corresponding to a relation with that relation $\mathsf{\textsl{attr-X/}}\mathcal{R}$. Furthermore, as introduced above, we use predicates to represent operators in the data quality as shown in Table \ref{tab:concept}.

\begin{table}[h]\footnotesize
%\scriptsize
\centering
\begin{tabular}{@{}lcl@{}}
\toprule
Concept    & Operator & Predicate \\ \midrule
Similarity & $\mathcal{S}_1[x_1]\approx \mathcal{S}_2[x_2]$        & $\mathsf{\textsl{similar}(x_1,x_2)}$ \\
Equality   & $y_1=y_2$ & $\mathsf{\textsl{equal-Y}(id_1, id_2)}$ \\
Matching   & $\mathcal{S}_1[y_1]\rightleftharpoons \mathcal{S}_2[y_2]$   & $\mathsf{\textsl{S1/match-Y/S2}(id_1, id_2)}$ \\ \bottomrule
\end{tabular}
\caption{Markov logic predicates used for data cleaning.}
\label{tab:concept}
\end{table}

%\anno{TOO GENERIC AND HIGH LEVEL, SOUNDS LIKE A COPY FROM AN ML TEXTBOOK. CONNECT THIS TO OUR PROBLEM! -> this is not a copy from any books.
% but you are correct, this should be connected to the backend of the method.
%MLNs are created by writing a set of first-order logic rules with weights, constructed using variables that range of objects in the domain of interest and predicates that represent relations between these objects. Predicates can be observed or hidden. \textit{Observed predicates} are relations between objects that can be seen in a given data set. $\mathsf{\textsl{attr-X}_1(id,v_1)}$, $\mathsf{\textsl{attr-X}_2(id,v_2)}$ $\dots$ $\mathsf{\textsl{attr-X}_n(id,v_n)}$  where $\mathsf{\textsl{attr-X}_i(id,v_i)}$ are observed predicates. In addition, an MLN may have a number of \textit{hidden predicates}, meaning that they are not observed in the input data, but can be inferred through rules. In our case, $\mathsf{\textsl{equal-Y}(id_1, id_2)}$ and $\mathsf{\textsl{S1/match-Y/S2}(id_1, id_2)}$ are hidden.  The goal of probabilistic inference is to infer the likelihood for observed and hidden predicates given a rule set and evidence. We define the MLN in such a way that reasoning about hidden predicates given evidence and data cleaning rules allows us to determine data repair operations.}

%\subsubsection{First-Order Logic Formulae}
%\note{Explain why we are doing all of this? What is the advantage?} 
We assume that integrity constraints have been already determined by using methods reviewed in \cite{liu2012discover} or specified by domain experts manually. To demonstrate the compilation of the data cleaning rules into Markov logic, have a look at the CFD from the motivation example in Section~\ref{sec:expl}. This CFD states that if any two tuples agree on attribute values for $\mathsf{\textsl{city}}$ and $\mathsf{\textsl{phone}}$, then the attribute values on $\mathsf{\textsl{street}}$ and $\mathsf{\textsl{zip}}$ should agree as well:
\begin{equation*}
\mathsf{fd: \textsc{transaction}([\textsl{city}, \textsl{phone}] \rightarrow [\textsl{street}, \textsl{zipcode}])}
\end{equation*}
\vspace*{-0.5cm}

To enable straightforward compilation, we assume that CFDs are provided in normal form. This means if $\psi(X \rightarrow Y_1,Y_2,\dots , T_p)$, then $\psi$ will be decomposed into several CFDs where $RHS(\psi)$ (right hand side of $\psi$) becomes a single attribute: $\psi_1(X \rightarrow Y_1 , T_p)$, $\psi_2(X \rightarrow Y_2 , T_p) \dots$. Following the normalization rule for functional dependencies, we split the $\mathsf{fd}$ rule into two rules:
\begin{flalign*}
& \mathsf{cfd_1: \textsc{transaction}([\textsl{city}, \textsl{phone}] \rightarrow [\textsl{street}], t_1=(\_, \_ \parallel  \_))}& \\
& \mathsf{cfd_2: \textsc{transaction}([\textsl{city}, \textsl{phone}] \rightarrow [\textsl{zipcode}], t_2=(\_, \_ \parallel  \_))}&
\end{flalign*}
\vspace*{-0.5cm}

According to~\cite{Fagin:1982:HCD:322344.322347} we can represent these $\mathsf{cfd_1}$ and $\mathsf{cfd_2}$ as two first-order logic formulas:
\begin{flalign*}
& \mathsf{1)~\forall~\textsl{city}, \textsl{phone}, \textsl{street}_1, \textsl{street}_2 }& \\
& \mathsf{\textsc{transaction}(\textsl{city}, \textsl{phone}, \textsl{street}_1)~\wedge}&\\
& \mathsf{\textsc{transaction}(\textsl{city}, \textsl{phone}, \textsl{street}_2) }& \\
& \mathsf{ \Rightarrow \textsl{street}_1=\textsl{street}_2 }& \\
& \mathsf{2) ~\forall~\textsl{city}, \textsl{phone}, \textsl{zip}_1, \textsl{zip}_2 }& \\
& \mathsf{ ~\textsc{transaction}(\textsl{city}, \textsl{phone}, \textsl{zip}_1) \wedge}& \\
& \mathsf{\textsc{transaction}(\textsl{city}, \textsl{phone}, \textsl{zip}_2) }& \\
& \mathsf{ \Rightarrow \textsl{zip}_1=\textsl{zip}_2 }&
\end{flalign*}
\vspace*{-0.3cm}

These formulas state that if two tuples of the \textsc{transaction} relation agree on the \textsl{city} and \textsl{phone} values, then their \textsl{street} and \textsl{zip} values should be the same. Once we have formulated our integrity constraints as first-order logic formulas, we translate them into Markov
logic syntax. We illustrate this with the first-order logic formulas $\mathsf{1)}$ and $\mathsf{2)}$ as defined above.
Given that every attribute from the schema $\mathsf{\textsc{transaction}}$ is expressed as a predicate, we need two predicates, 
namely $\mathsf{\textsl{city}(id, city)}$ and $\mathsf{\textsl{phone}(id, phone)}$ to encode the LHS of $\mathsf{fd}$. They indicate the values for the fields \textsl{city} and \textsl{phone} for each tuple. The full example is given in Table~\ref{tab:mlndeclare}, which shows all steps in predicate translation and how data from our example database is translated into grounded atoms. Additionally, we define two predicates for our data quality rule, namely $\mathsf{\textsl{equal-street}(id, id)}$ and $\mathsf{\textsl{equal-zip}(id, id)}$. These predicates model equality of two values of the attribute \textsl{street} respectively \textsl{zip} (as denoted by the $\mathsf{fd}$ rule above):

\begin{flalign*}
	& \mathsf{1)  \textsl{city}(id_1, city) \wedge \textsl{city}(id_2, city) \wedge \textsl{phone}(id_1, phone) }& \\ 
	& \mathsf{  \wedge \textsl{phone}(id_2, phone) \Rightarrow \textsl{equal-street}(id_1, id_2)} & \\
	& \mathsf{2)  \textsl{city}(id_1, city) \wedge \textsl{city}(id_2, city) \wedge \textsl{phone}(id_1, phone) }& \\ 
	& \mathsf{  \wedge \textsl{phone}(id_2, phone) \Rightarrow \textsl{equal-zip}(id_1, id_2)} & 
\end{flalign*}
\vspace*{-0.5cm}

Note that using the same arguments in predicates $\mathsf{\textsl{city}(id, city)}$ and $\mathsf{\textsl{phone}(id, phone)}$ encodes equality of the corresponding values in the Markov logic program. The two different tuples are distinguished by $\mathsf{id_1}$ and $\mathsf{id_2}$. 
These data quality rules form the basis for a Markov logic program. A particular advantage of Markov logic is its \textit{modularity} while modeling. We can create "complex" statistical models by combining various "atomic" models. Thereby, we consider each declared data quality rule as an "atomic" probabilistic model. In combination, these small models form a "compound" model. So far we have shown how to express integrity constraints in terms of the predicate calculus.
Having declared the data quality rules by capturing all correlations and constraints, we can now infer potential predicates, such as $\mathsf{\textsl{equal-street}(id, id)}$. This predicate holds the information about possible repairs on the attribute $\mathsf{\textsl{street}}$ in the \textsc{transactions} tables in our running example from Section~\ref{sec:expl}. Reasoning about such hidden predicates allows us to decide which attributes gets a particular repair. 

\begin{table}[t]\footnotesize
%\scriptsize
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}ll@{}}
\toprule
Phase                                                                             & Example                                                                                                                                                          \\ \midrule
1) Schema definition                                                              & \begin{tabular}[c]{@{}l@{}}$t_2$(item, firstname, lastname, street,\\ city, zipcode, phone)\end{tabular}                                                              \\ \midrule
\begin{tabular}[c]{@{}l@{}}2) Observed predicates \\ MLN declaration\end{tabular} & \begin{tabular}[c]{@{}l@{}}$\mathsf{\textsl{firstname}(id, value)}$ \\ $\mathsf{\textsl{lastname}(id, lastname)}$ \\ $\mathsf{\textsl{street}(id, street)}$ \\ $\mathsf{\textsl{city}(id, city)}$\\ $\mathsf{\textsl{zip}(id, code)}$ \\ $\mathsf{\textsl{phone}(id, num)}$\end{tabular} \\ \midrule
3) Data                                                                           & \begin{tabular}[c]{@{}l@{}}$t_2$(Galaxy 5, NULL, Miller, \\ 12 Hay St., NULL, 818, 11234)\end{tabular}                                                              \\ \midrule
4) Grounded (evidence) atoms                                                      & \begin{tabular}[c]{@{}l@{}} $\mathsf{\textsl{item}(2, Galaxy5)}$ \\ $\mathsf{\textsl{lastname}(2, Miller)}$ \\ $\mathsf{\textsl{street}(2, 12HaySt.)}$ \\ $\mathsf{\textsl{zip}(2, 818)}$ \\ $\mathsf{\textsl{phone}(2, 11234)}$\end{tabular}                         \\ \bottomrule
\end{tabular}}
\caption{\label{tab:mlndeclare} MLN declaration process and creation of grounded atoms for Tuple 2 in the \textsc{Transactions} example table.}
\end{table}
\subsection{Data Repair as Joint Inference}
\label{subsec:jointinference}
Markov logic defines a knowledge representation system that combines first-order logic as a declarative language with probabilistic graphical models (undirected Markov Networks MLNs) on top of which we perform probabilistic inference. Semantically, a MLN is a log-linear model, which defines the probability distribution over possible worlds, in our case all possible states in the database.

We create MLNs by writing a set of first-order logic rules (c.f., Section~\ref{sec:ml}) with weights by using predicates that represent relations between these objects. We distinguish between observed and hidden predicates. \textit{Observed predicates} are relations between objects which exist in a given dataset: $\mathsf{\textsl{attr-X}_1(id,v_1)}$, $\mathsf{\textsl{attr-X}_2(id,v_2)}$ $\dots$ $\mathsf{\textsl{attr-X}_n(id,v_n)}$ are observed predicates, which encode that $\mathsf{\textsl{attr-X}_i}$ has value $\mathsf{v_i}$ on tuple $\mathsf{id}$. In addition, an MLN may have a number of \textit{hidden predicates}, which are not present in the input data, but can be inferred through rules. In our case, $\mathsf{\textsl{equal-Y}(id_1, id_2)}$ and $\mathsf{\textsl{S1/match-Y/S2}(id_1, id_2)}$ are hidden.  The goal of probabilistic inference is to infer the likelihood for observed and hidden predicates given a rule set and evidence. We define the MLN in such a way that reasoning about hidden predicates given evidence and data cleaning rules allow us to determine data repair operations. In other words, we perform an inference task as a prediction for the data cleaning.
We create MLNs by writing a set of first-order logic rules (c.f., Section~\ref{sec:ml}) with weights by using predicates that represent relations between these objects. We distinguish between observed and hidden predicates. \textit{Observed predicates} are relations between objects which exist in a given dataset: $\mathsf{\textsl{attr-X}_1(id,v_1)}$, $\mathsf{\textsl{attr-X}_2(id,v_2)}$ $\dots$ $\mathsf{\textsl{attr-X}_n(id,v_n)}$ are observed predicates, which encode that $\mathsf{\textsl{attr-X}_i}$ has value $\mathsf{v_i}$ on tuple $\mathsf{id}$. In addition, an MLN may have a number of \textit{hidden predicates}, which are not present in the input data, but can be inferred through rules. In our case, $\mathsf{\textsl{equal-Y}(id_1, id_2)}$ and $\mathsf{\textsl{S1/match-Y/S2}(id_1, id_2)}$ are hidden.  The goal of probabilistic inference is to infer the likelihood for observed and hidden predicates given a rule set and evidence. We define the MLN in such a way that reasoning about hidden predicates given evidence and data cleaning rules allow us to determine data repair operations. In other words, we perform an inference task as a prediction for the data cleaning.

%%%%%%
An important step in our method is the \textit{grounding} of the MLN (in details illustrated in Figure \ref{fig:mlngrounding}). 
%\note{EXPLAIN THIS FIGURE IN DETAIL *PEITSCH*} - ok, but there is a lot of explanation here, and if we put everything here, then this will disturb the reading flow. Would it be acceptable to provide details in figure caption?! 
Because constraint-based rules are data agnostic~\cite{fan2012foundations}, we need to ``ground'' previously defined predicates with the available evidence, in our case the database to process. We take the content of the database (a set of tuples) and produce a set of observed predicates (so-called \textit{grounded atoms}). Let $n$ denote the number of schema attributes. Then we convert each tuple from the database into $n$ different Markov logic evidence atoms. For example, we translate transaction 2 in our running example into the following 6 grounded atoms: \textsl{item}(2, Galaxy 5), \textsl{firstname}(2, Max), \textsl{lastname}(2, Miller), \textsl{street}(2, 12 Hay St.), \textsl{zip}(2, 818) and \textsl{phone}(11234). These are examples of observed predicates for the dataset. Table~\ref{tab:mlndeclare} summarizes the process of the groundings creation. We use these groundings (or evidence atoms) in the joint inference for data cleaning. 
%%%%%
Given an MLN that models data cleaning rules, let $q \in \mathcal{L}^n$ denote a hidden predicate with $n$ \emph{literals} (random variables) $L_1,...,L_n$, where each literal $L_i$
has $2$ discrete states, $L_i = \lbrace 0,1 \rbrace$.
Then, the MLN is a joint distribution on  $L_1,...,L_n$ that 
is specified by a vector $\phi(q)$ of $d$ integer values, where
each element represents the number of true groundings of the 
corresponding literal in the formula and $d$ denotes the 
maximum number of literals in a given formula. Additionally, 
we have a weight/parameter vector $\theta \in \mathcal{R}^d$:
%\vspace{-3em}
\begin{equation*}
Pr \left( q | \theta \right) = 
\frac{1}{Z(\theta)} \exp\left( \langle \theta, \phi(q) \rangle  \right), 
Z(\theta) = \sum_{q \in \mathcal{L}^n}\exp\left( \langle \theta, \phi(q) \rangle  \right) 
\end{equation*}
%\vspace{-2em}
where $\langle \theta, \phi(q) \rangle$ denotes a dot product. 
$Z(\theta)$ is the normalization constant also called the 
\emph{partition function}. Since the partition function is a constant and the exponential is monotonic, finding the MAP assignment in our data cleaning problem is equivalent to finding the assignment $q_M$ that maximizes the probability $Pr \left( q | \theta \right)$.

The output of the inference are data repair operations; e.g., the hidden predicate \textsl{equal-street} may be determined to have (among other groundings) the following likely value: $\mathsf{\textsl{equal-street}(1, 3)}$, indicating that the \textsl{street} field for transaction 3 should have the same value as the \textsl{street} field of transaction 1. In this case, the data repair operation is to replace the \textsc{null} value in transaction 3 with the address ``1 Sun Dr.''. 
%\note{Shouldn't the MLN allow us to inspect the uncertainty in these decisions? Can you elaborate on that?}  I was thinking about this. Because we are using MAP, we assume that we take the most possible solution. otherwise we would need to explain all of the Markov logic + Markov networks here.
As we noted in the discussion of the running example, there is an interplay of different rules that affects the probability of hidden predicates. Therefore, the inference produces the most likely state of the entire Markov Logic Network with regards to all integrity constraints. The probabilities for the hidden predicates are therefore influenced by \textit{all} defined data quality rules. By running the inference over the entire database, we predict the most likely data repairs for our data set by determining the most likely grounding of the hidden predicates and this reduces to computing \textsc{map} inference on the MLN model.      

The difficulty in designing algorithms for MAP inference arises when finding an efficient way to reason about the large number of possible assignments to the variables in the model. In terms of theoretical guarantees, inference is NP-hard or or computationally intractable and as proven by \cite{Shimony1994} in many cases cannot be approximated. Amongst the numerous techniques that are available to solve MAP inference problems, we chose to cast the inference problem as an \emph{integer linear program} (ILP)\cite{Sontag10approximateinference}. Another competitive approach called \emph{message passing} perform \emph{belief propagation} along the edges of the graphical model. Although, message passing is straightforward to implement it has troubles converging \cite{schwing2011distributed, felzenszwalb2006efficient, pritch2009shift} and tends not to give as good results as ILP~\cite{NoessnerNS13}. 

